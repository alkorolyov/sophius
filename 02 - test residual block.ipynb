{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "import sophius.templates as tmpl\n",
    "import sophius.utils as utils\n",
    "import sophius.dataload as dload\n",
    "from sophius.train import train_express_gpu, validate_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\n"
    }
   ],
   "source": [
    "cifar10 = dset.CIFAR10('data//CIFAR10', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "cifar_gpu = dload.cifar_to_gpu(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VAL = 1024\n",
    "loader_gpu = dload.get_loader_gpu(cifar_gpu, NUM_VAL, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self._shortcut(x) + self._block(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetBlock(_ResNetBlock):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=False)\n",
    "        conv2 = nn.Conv2d(in_channels=out_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=3, \n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=False)\n",
    "        bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        self._block = nn.Sequential(conv1, bn1, relu,\n",
    "                                    conv2, bn2)\n",
    "        self._shortcut = nn.Identity()\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        #         nn.init.constant_(m.weight, 1)\n",
    "        #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ResNetSkipBlock(_ResNetBlock):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=2,\n",
    "                          padding=1,\n",
    "                          bias=False)\n",
    "        conv2 = nn.Conv2d(in_channels=out_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=False)\n",
    "        bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        self._block = nn.Sequential(conv1, bn1, relu,\n",
    "                                    conv2, bn2)\n",
    "        conv_shortcut = nn.Conv2d(in_channels=in_channels,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=1, \n",
    "                                  stride=2,\n",
    "                                  padding=0,\n",
    "                                  bias=False)\n",
    "        bn3 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self._shortcut = nn.Sequential(conv_shortcut, bn3)\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        #         nn.init.constant_(m.weight, 1)\n",
    "        #         nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([32, 3, 32, 32])\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n          Identity-1            [-1, 3, 32, 32]               0\n            Conv2d-2            [-1, 3, 32, 32]              81\n       BatchNorm2d-3            [-1, 3, 32, 32]               6\n              ReLU-4            [-1, 3, 32, 32]               0\n            Conv2d-5            [-1, 3, 32, 32]              81\n       BatchNorm2d-6            [-1, 3, 32, 32]               6\n              ReLU-7            [-1, 3, 32, 32]               0\n================================================================\nTotal params: 174\nTrainable params: 174\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.16\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.18\n----------------------------------------------------------------\nNone\n"
    }
   ],
   "source": [
    "x = torch.randn(32, 3, 32, 32).cuda()\n",
    "block = ResNetBlock(3, 3).cuda()\n",
    "out = block(x)\n",
    "\n",
    "print(out.shape)\n",
    "print(summary(block, (3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([32, 64, 16, 16])\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 16, 16]             192\n       BatchNorm2d-2           [-1, 64, 16, 16]             128\n            Conv2d-3           [-1, 64, 16, 16]           1,728\n       BatchNorm2d-4           [-1, 64, 16, 16]             128\n              ReLU-5           [-1, 64, 16, 16]               0\n            Conv2d-6           [-1, 64, 16, 16]          36,864\n       BatchNorm2d-7           [-1, 64, 16, 16]             128\n              ReLU-8           [-1, 64, 16, 16]               0\n================================================================\nTotal params: 39,168\nTrainable params: 39,168\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 1.00\nParams size (MB): 0.15\nEstimated Total Size (MB): 1.16\n----------------------------------------------------------------\nNone\n"
    }
   ],
   "source": [
    "x = torch.randn(32, 3, 32, 32).cuda()\n",
    "block = ResNetSkipBlock(3, 64).cuda()\n",
    "out = block(x)\n",
    "print(out.shape)\n",
    "print(summary(block, (3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ResNetSkipBlock(\n  (relu): ReLU(inplace=True)\n  (_block): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (_shortcut): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\n"
    }
   ],
   "source": [
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([32, 10])\n"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=3,\n",
    "                 out_channels=64,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 padding=1,\n",
    "                 bias=False)\n",
    "maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "relu = nn.ReLU(inplace=False)\n",
    "gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "flat = tmpl.Flatten()\n",
    "fc = nn.Linear(512, 10)\n",
    "model = nn.Sequential(conv, nn.BatchNorm2d(64), relu,\n",
    "                      ResNetBlock(64, 64), ResNetBlock(64, 64),\n",
    "                      ResNetSkipBlock(64, 128), ResNetBlock(128, 128),\n",
    "                      ResNetSkipBlock(128, 256), ResNetBlock(256, 256),\n",
    "                      ResNetSkipBlock(256, 512), ResNetBlock(512, 512),\n",
    "                      gap, flat, fc)\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([32, 64, 16, 16])\n"
    }
   ],
   "source": [
    "x = torch.randn(32, 3, 32, 32)\n",
    "out = conv(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Finished in 23.7s \nval_acc: 0.609, train_acc: 0.641\nFinished in 24.3s \nval_acc: 0.583, train_acc: 0.614\nFinished in 24.2s \nval_acc: 0.602, train_acc: 0.646\nFinished in 23.1s \nval_acc: 0.628, train_acc: 0.687\nFinished in 24.1s \nval_acc: 0.601, train_acc: 0.632\nFinished in 23.8s \nval_acc: 0.631, train_acc: 0.672\nFinished in 25.0s \nval_acc: 0.600, train_acc: 0.621\nFinished in 24.8s \nval_acc: 0.596, train_acc: 0.638\nFinished in 24.8s \nval_acc: 0.609, train_acc: 0.656\nFinished in 24.8s \nval_acc: 0.607, train_acc: 0.625\n10 iters: 24.3s \nval: 0.607 +- 0.014 train: 0.643 +- 0.022\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(24.250173377990723, 0.60654296875, 0.6431640625)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# default weight init\n",
    "model_gpu = model.cuda()\n",
    "validate_model(model=model_gpu,\n",
    "                train=True,\n",
    "                loader=loader_gpu,\n",
    "                milestones=[],\n",
    "                num_iter=10,\n",
    "                num_epoch=1,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Finished in 22.7s \nval_acc: 0.604, train_acc: 0.647\nFinished in 24.6s \nval_acc: 0.629, train_acc: 0.637\nFinished in 23.6s \nval_acc: 0.607, train_acc: 0.654\nFinished in 22.6s \nval_acc: 0.587, train_acc: 0.641\nFinished in 22.5s \nval_acc: 0.603, train_acc: 0.625\nFinished in 22.8s \nval_acc: 0.518, train_acc: 0.564\nFinished in 22.8s \nval_acc: 0.600, train_acc: 0.606\nFinished in 22.9s \nval_acc: 0.516, train_acc: 0.561\nFinished in 23.1s \nval_acc: 0.637, train_acc: 0.675\nFinished in 24.5s \nval_acc: 0.555, train_acc: 0.579\n10 iters: 23.2s \nval: 0.585 +- 0.040 train: 0.619 +- 0.038\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23.197196984291075, 0.58544921875, 0.6189453125)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# custom weight init\n",
    "model_gpu = model.cuda()\n",
    "validate_model(model=model_gpu,\n",
    "               train=True,\n",
    "               loader=loader_gpu,\n",
    "               milestones=[],\n",
    "               num_iter=10,\n",
    "               num_epoch=1,\n",
    "               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 18.9s \n",
      "val_acc: 0.516, train_acc: 0.528\n"
     ]
    }
   ],
   "source": [
    "t, val_acc, train_acc = train_express_gpu(model = resnet18,\n",
    "                                          train = True,\n",
    "                                          loader = loader_gpu,\n",
    "                                          milestones = [],\n",
    "                                          num_epoch = 1,\n",
    "                                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 16, 16]           1,728\n       BatchNorm2d-2           [-1, 64, 16, 16]             128\n              ReLU-3           [-1, 64, 16, 16]               0\n          Identity-4           [-1, 64, 16, 16]               0\n            Conv2d-5           [-1, 64, 16, 16]          36,864\n       BatchNorm2d-6           [-1, 64, 16, 16]             128\n              ReLU-7           [-1, 64, 16, 16]               0\n            Conv2d-8           [-1, 64, 16, 16]          36,864\n       BatchNorm2d-9           [-1, 64, 16, 16]             128\n             ReLU-10           [-1, 64, 16, 16]               0\n      ResNetBlock-11           [-1, 64, 16, 16]               0\n         Identity-12           [-1, 64, 16, 16]               0\n           Conv2d-13           [-1, 64, 16, 16]          36,864\n      BatchNorm2d-14           [-1, 64, 16, 16]             128\n             ReLU-15           [-1, 64, 16, 16]               0\n           Conv2d-16           [-1, 64, 16, 16]          36,864\n      BatchNorm2d-17           [-1, 64, 16, 16]             128\n             ReLU-18           [-1, 64, 16, 16]               0\n      ResNetBlock-19           [-1, 64, 16, 16]               0\n           Conv2d-20            [-1, 128, 8, 8]           8,192\n      BatchNorm2d-21            [-1, 128, 8, 8]             256\n           Conv2d-22            [-1, 128, 8, 8]          73,728\n      BatchNorm2d-23            [-1, 128, 8, 8]             256\n             ReLU-24            [-1, 128, 8, 8]               0\n           Conv2d-25            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-26            [-1, 128, 8, 8]             256\n             ReLU-27            [-1, 128, 8, 8]               0\n  ResNetSkipBlock-28            [-1, 128, 8, 8]               0\n         Identity-29            [-1, 128, 8, 8]               0\n           Conv2d-30            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-31            [-1, 128, 8, 8]             256\n             ReLU-32            [-1, 128, 8, 8]               0\n           Conv2d-33            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-34            [-1, 128, 8, 8]             256\n             ReLU-35            [-1, 128, 8, 8]               0\n      ResNetBlock-36            [-1, 128, 8, 8]               0\n           Conv2d-37            [-1, 256, 4, 4]          32,768\n      BatchNorm2d-38            [-1, 256, 4, 4]             512\n           Conv2d-39            [-1, 256, 4, 4]         294,912\n      BatchNorm2d-40            [-1, 256, 4, 4]             512\n             ReLU-41            [-1, 256, 4, 4]               0\n           Conv2d-42            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-43            [-1, 256, 4, 4]             512\n             ReLU-44            [-1, 256, 4, 4]               0\n  ResNetSkipBlock-45            [-1, 256, 4, 4]               0\n         Identity-46            [-1, 256, 4, 4]               0\n           Conv2d-47            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-48            [-1, 256, 4, 4]             512\n             ReLU-49            [-1, 256, 4, 4]               0\n           Conv2d-50            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-51            [-1, 256, 4, 4]             512\n             ReLU-52            [-1, 256, 4, 4]               0\n      ResNetBlock-53            [-1, 256, 4, 4]               0\n           Conv2d-54            [-1, 512, 2, 2]         131,072\n      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n           Conv2d-56            [-1, 512, 2, 2]       1,179,648\n      BatchNorm2d-57            [-1, 512, 2, 2]           1,024\n             ReLU-58            [-1, 512, 2, 2]               0\n           Conv2d-59            [-1, 512, 2, 2]       2,359,296\n      BatchNorm2d-60            [-1, 512, 2, 2]           1,024\n             ReLU-61            [-1, 512, 2, 2]               0\n  ResNetSkipBlock-62            [-1, 512, 2, 2]               0\n         Identity-63            [-1, 512, 2, 2]               0\n           Conv2d-64            [-1, 512, 2, 2]       2,359,296\n      BatchNorm2d-65            [-1, 512, 2, 2]           1,024\n             ReLU-66            [-1, 512, 2, 2]               0\n           Conv2d-67            [-1, 512, 2, 2]       2,359,296\n      BatchNorm2d-68            [-1, 512, 2, 2]           1,024\n             ReLU-69            [-1, 512, 2, 2]               0\n      ResNetBlock-70            [-1, 512, 2, 2]               0\nAdaptiveAvgPool2d-71            [-1, 512, 1, 1]               0\n          Flatten-72                  [-1, 512]               0\n           Linear-73                   [-1, 10]           5,130\n================================================================\nTotal params: 11,173,962\nTrainable params: 11,173,962\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 4.24\nParams size (MB): 42.63\nEstimated Total Size (MB): 46.88\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(model.cuda(), (3, 32, 32))\n",
    "# 11.181.888‬\n",
    "# 11.176.512 resnet 18\n",
    "# diff 5376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 16, 16]           9,408\n       BatchNorm2d-2           [-1, 64, 16, 16]             128\n              ReLU-3           [-1, 64, 16, 16]               0\n         MaxPool2d-4             [-1, 64, 8, 8]               0\n            Conv2d-5             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-6             [-1, 64, 8, 8]             128\n              ReLU-7             [-1, 64, 8, 8]               0\n            Conv2d-8             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-9             [-1, 64, 8, 8]             128\n             ReLU-10             [-1, 64, 8, 8]               0\n       BasicBlock-11             [-1, 64, 8, 8]               0\n           Conv2d-12             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-13             [-1, 64, 8, 8]             128\n             ReLU-14             [-1, 64, 8, 8]               0\n           Conv2d-15             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-16             [-1, 64, 8, 8]             128\n             ReLU-17             [-1, 64, 8, 8]               0\n       BasicBlock-18             [-1, 64, 8, 8]               0\n           Conv2d-19            [-1, 128, 4, 4]          73,728\n      BatchNorm2d-20            [-1, 128, 4, 4]             256\n             ReLU-21            [-1, 128, 4, 4]               0\n           Conv2d-22            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-23            [-1, 128, 4, 4]             256\n           Conv2d-24            [-1, 128, 4, 4]           8,192\n      BatchNorm2d-25            [-1, 128, 4, 4]             256\n             ReLU-26            [-1, 128, 4, 4]               0\n       BasicBlock-27            [-1, 128, 4, 4]               0\n           Conv2d-28            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-29            [-1, 128, 4, 4]             256\n             ReLU-30            [-1, 128, 4, 4]               0\n           Conv2d-31            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-32            [-1, 128, 4, 4]             256\n             ReLU-33            [-1, 128, 4, 4]               0\n       BasicBlock-34            [-1, 128, 4, 4]               0\n           Conv2d-35            [-1, 256, 2, 2]         294,912\n      BatchNorm2d-36            [-1, 256, 2, 2]             512\n             ReLU-37            [-1, 256, 2, 2]               0\n           Conv2d-38            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-39            [-1, 256, 2, 2]             512\n           Conv2d-40            [-1, 256, 2, 2]          32,768\n      BatchNorm2d-41            [-1, 256, 2, 2]             512\n             ReLU-42            [-1, 256, 2, 2]               0\n       BasicBlock-43            [-1, 256, 2, 2]               0\n           Conv2d-44            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-45            [-1, 256, 2, 2]             512\n             ReLU-46            [-1, 256, 2, 2]               0\n           Conv2d-47            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-48            [-1, 256, 2, 2]             512\n             ReLU-49            [-1, 256, 2, 2]               0\n       BasicBlock-50            [-1, 256, 2, 2]               0\n           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n             ReLU-53            [-1, 512, 1, 1]               0\n           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n           Conv2d-56            [-1, 512, 1, 1]         131,072\n      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n             ReLU-58            [-1, 512, 1, 1]               0\n       BasicBlock-59            [-1, 512, 1, 1]               0\n           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n             ReLU-62            [-1, 512, 1, 1]               0\n           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n             ReLU-65            [-1, 512, 1, 1]               0\n       BasicBlock-66            [-1, 512, 1, 1]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 1.29\nParams size (MB): 44.59\nEstimated Total Size (MB): 45.90\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(resnet18, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n  (3): ResNetBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Identity()\n  )\n  (4): ResNetBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Identity()\n  )\n  (5): ResNetSkipBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (6): ResNetBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Identity()\n  )\n  (7): ResNetSkipBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (8): ResNetBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Identity()\n  )\n  (9): ResNetSkipBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (10): ResNetBlock(\n    (relu): ReLU(inplace=True)\n    (_block): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (_shortcut): Identity()\n  )\n  (11): AdaptiveAvgPool2d(output_size=(1, 1))\n  (12): Flatten()\n  (13): Linear(in_features=512, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('sophius': conda)",
   "language": "python",
   "name": "python37664bitsophiuscondae165756781f44b90be460d9d1813feed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}